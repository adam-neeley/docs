#+title: Test Time Training
#+author: Adam Neeley
This is my analysis of the test time training (TTT) paper found at
https://ekinakyurek.github.io/papers/ttt.pdf.

The git repository that contains the ideas found in the paper is located at https://github.com/ekinakyurek/marc.
* Introduction
The core idea of /test time training/ (TTT) is to temporarily finetune an LLM based on previously unseen input data.
This approach has /doubled the accuracy/ of fully-neural approaches from 25% to 53% accuracy.
When combined with /program synthesis/, this increases to 62%.
62% accuracy is roughly average human performance, but it falls short of expert human performance (93%).
** Definition
#+begin_quote
*Test time training*: updating model parameters temporarily during inference using a loss derived from input data.
#+end_quote
** Crucial parts
1. *Initial finetuning* on similar tasks generated at test time.
2. *Leave-one-out* generation strategy for constructing test-time dataset.
3. *Per-instance* adapter training.
4. *Self-consistency* approach under invertible transformations.
** Two methods
The paper presents two main categories for ARC solvers:
*** Program Synthesis
1. Find transformation $f$.
2. Apply to test example.
*** Fully Neural
1. Directly predict the test output, only implicitly reasoning of the underlying transformation.
* Conclusion
1. The paper presents a novel approach to TTT that leverages the strengths of LLMs and pre-trained models.
2. TTT is not AGI.
** Citation
@article{ekinakyurek2023ttt,
  title={Test Time Training for Large Language Models}, References
#+begin_bibliography:
#+end_bibliography:
